{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f6082e-f5c9-4ee1-8940-4ffcd4cd8a8d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Let's take a look at what's inside the datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36987a26-17a2-4a7a-8d90-93bade22b011",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Original Datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5af3f7-e436-40ed-863a-59182ebb257d",
   "metadata": {},
   "source": [
    "What's the label distribution look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "917d937e-9f43-44d2-9cc1-8b17b9ce52f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    7580\n",
      "0    7580\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"my_datasets/SMOTE-PCL.csv\")\n",
    "#df_test = pd.read_csv(\"my_datasets/Founta/new_Founta_non_hateful_synthetic_data.csv\")\n",
    "\n",
    "label_counts = df['labels'].value_counts()\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c90944e-cd69-4954-be7c-e4da64ce83c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'text' entries contained in both files: 75\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_common_text_entries(file1, file2):\n",
    "    # Read the CSV files\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "\n",
    "    # Get the 'text' entries in both files\n",
    "    text_entries_file1 = set(df1['text'])\n",
    "    text_entries_file2 = set(df2['text'])\n",
    "\n",
    "    # Find the common 'text' entries\n",
    "    common_text_entries = text_entries_file1.intersection(text_entries_file2)\n",
    "\n",
    "    # Get the number of common 'text' entries\n",
    "    num_common_text_entries = len(common_text_entries)\n",
    "\n",
    "    return num_common_text_entries\n",
    "\n",
    "# Specify the paths to the CSV files\n",
    "file1_path = 'my_datasets/GermEval/GE_train_original.csv'\n",
    "file2_path = 'my_datasets/GermEval/GE_test_original.csv'\n",
    "\n",
    "# Call the function to count the common 'text' entries\n",
    "common_text_entries_count = count_common_text_entries(file1_path, file2_path)\n",
    "\n",
    "print(f\"Number of 'text' entries contained in both files: {common_text_entries_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f1df27-7f4f-4a3b-a50f-42114a1b436b",
   "metadata": {},
   "source": [
    "How many entries are in both train and test sets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e792e9-8815-4d5a-8d93-26746225500f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's create out-of-domain Hate Speech datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "53f72526-4408-41f7-9522-ddf59c55238f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47/27547366.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_new = df3.append([df2, df1], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My coworker told me he was a pansexual &amp;amp; e...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Your straightforward communication isn't nearl...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @Fact: Fake friends are no different than s...</td>\n",
       "      <td>0</td>\n",
       "      <td>712.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thank You @RealDonaldTrump  @POTUS Enforcing t...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My friends kids are being shitheads and they j...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18647</th>\n",
       "      <td>Women, ever considered working for Reserve Ban...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18648</th>\n",
       "      <td>@WeirdNegro Made me wonder, if it was grape ju...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18649</th>\n",
       "      <td>Congress must hold the administration accounta...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18650</th>\n",
       "      <td>@BrandNewKer Wtf is that skank bitch Kate here...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18651</th>\n",
       "      <td>@SunnyLeone @DanielWeber99 you bitch. You fuck...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18652 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  labels  Unnamed: 0\n",
       "0      My coworker told me he was a pansexual &amp; e...       1         NaN\n",
       "1      Your straightforward communication isn't nearl...       0         NaN\n",
       "2      RT @Fact: Fake friends are no different than s...       0       712.0\n",
       "3      Thank You @RealDonaldTrump  @POTUS Enforcing t...       1         NaN\n",
       "4      My friends kids are being shitheads and they j...       0         NaN\n",
       "...                                                  ...     ...         ...\n",
       "18647  Women, ever considered working for Reserve Ban...       0         NaN\n",
       "18648  @WeirdNegro Made me wonder, if it was grape ju...       0         NaN\n",
       "18649  Congress must hold the administration accounta...       0         NaN\n",
       "18650  @BrandNewKer Wtf is that skank bitch Kate here...       1         NaN\n",
       "18651  @SunnyLeone @DanielWeber99 you bitch. You fuck...       1         NaN\n",
       "\n",
       "[18652 rows x 3 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"my_datasets/Davidson/davidson_original_train_undersampled.csv\")\n",
    "df2 = pd.read_csv(\"my_datasets/Founta/founta_original_train_undersampled.csv\")\n",
    "df3 = pd.read_csv(\"my_datasets/HatEval/HatEval_train_undersampled.csv\")\n",
    "#df4 = pd.read_csv(\"my_datasets/Stormfront/original/Stormfront_train_undersampled.csv\")\n",
    "\n",
    "# Append df2 and df3 separately to df1\n",
    "df_new = df3.append([df2, df1], ignore_index=True)\n",
    "\n",
    "# Shuffle the rows in the merged dataframe\n",
    "df_new = df_new.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "262a0f85-297b-4213-b86e-2cd0354bb601",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv(\"my_datasets/us_Davidson_Founta_HatEval.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3086e99a-590f-4d17-840c-95802f8b7816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47/1622224311.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_new = df3.append([df1, df2], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the united nations refugee agency says that th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the european parliament has voted in favor of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the most important thing for the eu to do is t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the eu must close its borders to all refugees,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the eu is being overrun by millions of muslim ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58747</th>\n",
       "      <td>the united nation's refugee agency has urged t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58748</th>\n",
       "      <td>the us has said it is \" deeply concerned \" by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58749</th>\n",
       "      <td>the u. s. has spent $ 14. 5 billion on refugee...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58750</th>\n",
       "      <td>the biggest loser is back! get ready for a new...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58751</th>\n",
       "      <td>this article is about the city in iran. for ot...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58752 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  labels\n",
       "0      the united nations refugee agency says that th...       0\n",
       "1      the european parliament has voted in favor of ...       0\n",
       "2      the most important thing for the eu to do is t...       1\n",
       "3      the eu must close its borders to all refugees,...       1\n",
       "4      the eu is being overrun by millions of muslim ...       1\n",
       "...                                                  ...     ...\n",
       "58747  the united nation's refugee agency has urged t...       0\n",
       "58748  the us has said it is \" deeply concerned \" by ...       0\n",
       "58749  the u. s. has spent $ 14. 5 billion on refugee...       0\n",
       "58750  the biggest loser is back! get ready for a new...       0\n",
       "58751  this article is about the city in iran. for ot...       0\n",
       "\n",
       "[58752 rows x 2 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"my_datasets/Davidson/synthetic_davidson_undersampled.csv\")\n",
    "df2 = pd.read_csv(\"my_datasets/Founta/founta_undersampled_synthetic.csv\")\n",
    "df3 = pd.read_csv(\"my_datasets/HatEval/synthetic/HatEval_synthetic_undersampled.csv\")\n",
    "#df4 = pd.read_csv(\"my_datasets/Stormfront/SF_synthetic_undersampled.csv\")\n",
    "\n",
    "# Append df2 and df3 separately to df1\n",
    "df_new = df3.append([df1, df2], ignore_index=True)\n",
    "\n",
    "# Shuffle the rows in the merged dataframe\n",
    "df_new = df_new.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b6049e5-4248-49fd-b6c7-19cfae90e9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1150/241016309.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_new = df1.append([df2], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.814465</td>\n",
       "      <td>0.814375</td>\n",
       "      <td>1.101889</td>\n",
       "      <td>0.814317</td>\n",
       "      <td>0.814579</td>\n",
       "      <td>4.2946</td>\n",
       "      <td>74.046</td>\n",
       "      <td>9.314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.792453</td>\n",
       "      <td>0.791786</td>\n",
       "      <td>1.052628</td>\n",
       "      <td>0.791786</td>\n",
       "      <td>0.791786</td>\n",
       "      <td>4.2860</td>\n",
       "      <td>74.195</td>\n",
       "      <td>9.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.829653</td>\n",
       "      <td>0.829366</td>\n",
       "      <td>0.886287</td>\n",
       "      <td>0.831052</td>\n",
       "      <td>0.829339</td>\n",
       "      <td>4.2578</td>\n",
       "      <td>74.451</td>\n",
       "      <td>9.394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.801887</td>\n",
       "      <td>0.800448</td>\n",
       "      <td>1.214740</td>\n",
       "      <td>0.800036</td>\n",
       "      <td>0.801006</td>\n",
       "      <td>4.2666</td>\n",
       "      <td>74.532</td>\n",
       "      <td>9.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.839117</td>\n",
       "      <td>0.836771</td>\n",
       "      <td>0.817704</td>\n",
       "      <td>0.836074</td>\n",
       "      <td>0.837624</td>\n",
       "      <td>4.2667</td>\n",
       "      <td>74.296</td>\n",
       "      <td>9.375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  eval_accuracy   eval_f1  eval_loss  eval_precision  eval_recall  \\\n",
       "0    5.0       0.814465  0.814375   1.101889        0.814317     0.814579   \n",
       "1    5.0       0.792453  0.791786   1.052628        0.791786     0.791786   \n",
       "2    5.0       0.829653  0.829366   0.886287        0.831052     0.829339   \n",
       "3    5.0       0.801887  0.800448   1.214740        0.800036     0.801006   \n",
       "4    5.0       0.839117  0.836771   0.817704        0.836074     0.837624   \n",
       "\n",
       "   eval_runtime  eval_samples_per_second  eval_steps_per_second  \n",
       "0        4.2946                   74.046                  9.314  \n",
       "1        4.2860                   74.195                  9.333  \n",
       "2        4.2578                   74.451                  9.394  \n",
       "3        4.2666                   74.532                  9.375  \n",
       "4        4.2667                   74.296                  9.375  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"Logs/PCL_bert_uncased_us_fold5_metrics.csv\")\n",
    "df2 = pd.read_csv(\"Logs/PCL/PCL_bert_uncased_us_metrics.csv\")\n",
    "#df3 = pd.read_csv(\"Logs/bert_uncased_Founta_metrics_0.csv\")\n",
    "\n",
    "df1 = df1.drop(columns=['Unnamed: 0'])\n",
    "df2 = df2.drop(columns=[\"Unnamed: 0\"])\n",
    "#df3 = df3.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "df_new = df1.append([df2], ignore_index=True)\n",
    "\n",
    "df_new.to_csv(\"Logs/PCL/PCL_bert_uncased_us_metrics.csv\", index=None)\n",
    "df_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cba36fb-d9bb-447b-af71-8387bfcdcdb4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Synthetic Datsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f72b9461-8722-4178-82a5-c5f94ef1fb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text     a group of women from the northern part of the country have come up with an innovative idea to help vulnerable women and children from across the region. the group, called'women of the north'has come up with a unique project that will see them visiting refugee camps to hand over'smart'mobile\n",
      "Label                                                                                                                                                                                                                                                                                                 LABEL_1\n",
      "Score                                                                                                                                                                                                                                                                                                 0.99671\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"my_datasets/dontpatronizeme_v1.4/synthetic/classified_trunc_patronizing_synthetic_data.csv\")\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(df.iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bb27248c-0413-49e6-8f1c-2acf77dac2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                       Text  \\\n",
      "33                             you must be thinking of the romanian woman who raped 100s of children in the uk. she was a single mother with a drug problem   \n",
      "106                                                if you're white and you're under 18 and you're not in college, you're probably not getting the education   \n",
      "161    the united nations is a corrupt organization that serves the interests of the jews and the marxist agenda. the un was founded to serve the interests   \n",
      "180                             the video shows a group of negroes attacking a white woman in a parking lot, and then two other white men coming to her aid   \n",
      "188                                        the term ` ` white'' refers to a race of people and not an ethnicity as such. the term ` ` white'' is a race and   \n",
      "...                                                                                                                                                     ...   \n",
      "40025           london ( reuters ) - britain will have to start leaving the european union if it rejects a brexit deal that respects the referendum result,   \n",
      "40028                                              the state of texas has just passed a law that makes it a crime to give food to an anorexic. i wonder how   \n",
      "40041                   the video shows a group of white people beating up a group of black people for no apparent reason. the video shows a group of white   \n",
      "40093                              the last time i checked, homosexuality is not mentioned in the bible. it's not mentioned in the torah either. so why are   \n",
      "40181         washington - - the obama administration is signaling it is ready to go all in on its nuclear weapons policy - - and abandon the last vestiges   \n",
      "\n",
      "         Label     Score  \n",
      "33     LABEL_1  0.626521  \n",
      "106    LABEL_0  0.656852  \n",
      "161    LABEL_1  0.596620  \n",
      "180    LABEL_1  0.524581  \n",
      "188    LABEL_1  0.685909  \n",
      "...        ...       ...  \n",
      "40025  LABEL_1  0.550440  \n",
      "40028  LABEL_1  0.659497  \n",
      "40041  LABEL_0  0.678921  \n",
      "40093  LABEL_0  0.567554  \n",
      "40181  LABEL_1  0.638071  \n",
      "\n",
      "[859 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "selected_rows = df[(df['Score'] >= 0.5) & (df['Score'] <= 0.7)]\n",
    "\n",
    "print(selected_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "773d2e5a-3b75-43f1-988f-6d5044ef539a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Text  \\\n",
      "1      <h> god is greater than any obstacle, any problem, any difficulty that we may be facing at this time. he is greater than any sickness, greater than any pain, greater than any heartbreak, any kind of suffering that we may be going through. he is greater than any fear\\t\\ni am a single mother of two children. the eldest is a daughter who is currently in high school and the youngest is a son who is in pre-school. i have been a maid, a nanny, a babysitter and a mother all by myself. i have also worked as a cash\\t\\ni was born in the wrong country. i was raised in the wrong religion. i was educated in the wrong school. i had the wrong set of friends. i dated the wrong type of person. i had the wrong type of job. i lived in the wrong city. i went to the wrong\\t\\n- Advertisement -\\n\\n Whenever the powerful and privileged take refuge in the'natural order'of things, and the poor and vulnerable are told to'get used to'the situation, and'accept'their'fate, it is time to say no. Whenever the powerful and privileged tell the   \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   a group of women from the northern part of the country have come up with an innovative idea to help vulnerable women and children from across the region. the group, called'women of the north'has come up with a unique project that will see them visiting refugee camps to hand over'smart'mobile   \n",
      "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 in his weekly column, james Carvel notes that there are two types of people in this world : those who are privileged to have a roof over their head and those who are homeless. he suggests that those who are privileged to have a roof over their heads should help those who are homeless by giving   \n",
      "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           the project : a social enterprise, the mission is to rescue and rehabilitate homeless children and provide them with a safe, loving and permanent home. every child that comes through our doors is loved, valued and cared for. the children are educated, happy and have a bright future. the project : a   \n",
      "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             How can a society that allows a child to die of starvation treat its most vulnerable with such cruelty and call it progress? asks the architect ambedkar while describing the conditions that the dalits ( formerly known as the untouchables ) were forced to live under   \n",
      "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ...   \n",
      "44628                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ENGLEWOOD, Colo. (CBS4) - Two Englewood women are trying to help homeless women. they are concentrating on providing them with used clothing and quantities of food. community reporter lethrin ramon has the story.\\ncosette mark and monica gonzales both of eng   \n",
      "44631                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Saad al- harbour, the main character in the show, is the classic example of a hopeless millennial soul who has failed in life, due to a chaotic and action-less existence devoid of any dreams or hopes. the show centres around him, but along the way he meets a cast of odd   \n",
      "44632                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \"[...] Somehow in an effort to be helpful, a well-intentioned commenter on my first book TLS, which is published by Cornell, suggested that the'hundreds of pounds of books and equipment'that we received from Toys for Tots must have been a lot for a needy family. But as   \n",
      "44639                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Corrections Officer Forrest Welker Helfrich has been described by friends as an avid football and running fan who sacrificed his nine-month sabbatical to return to prison to pursue a dream to help rehabilitate prisoners. the associate warden ( asst warden ) of oklahoma's structural   \n",
      "44640                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Courtesy of Heroes Resource International, LLC (HRDI ) a non-profit charity, our partner Heroes Home for Kids is able to offer extremely poor families in the americas the opportunity to have their children blessed with the gift of hearing. by funding our partner the dutch Courage Foundation, we are able   \n",
      "\n",
      "         Label     Score  \n",
      "1      LABEL_1  0.997803  \n",
      "2      LABEL_1  0.996710  \n",
      "7      LABEL_1  0.998382  \n",
      "10     LABEL_1  0.998379  \n",
      "14     LABEL_1  0.996353  \n",
      "...        ...       ...  \n",
      "44628  LABEL_1  0.965866  \n",
      "44631  LABEL_1  0.997936  \n",
      "44632  LABEL_1  0.998179  \n",
      "44639  LABEL_1  0.997101  \n",
      "44640  LABEL_1  0.998291  \n",
      "\n",
      "[10578 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "selected_row = df[(df['Label'] == 'LABEL_1')]\n",
    "\n",
    "print(selected_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916665f-5d15-4ff3-9c4c-9b7a8f0e2d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31f28cfb-05c7-455d-abf8-b32951b7dc1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## And here's other, not-named code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a47ec-3a93-40bb-9703-6aea24ab1685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddf4bf4a-e818-4a3c-966b-4a7feb9490f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "undersample the input .csv to the minority class\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the original CSV file\n",
    "original_df = pd.read_csv('my_datasets/dontpatronizeme_v1.4/synthetic/composite_synthetic_pcl_data.csv')  # Replace 'original_dataset.csv' with your file name\n",
    "\n",
    "# Separate the dataset into two classes based on labels\n",
    "class_0 = original_df[original_df['labels'] == 0]\n",
    "class_1 = original_df[original_df['labels'] == 1]\n",
    "\n",
    "# Calculate the size of the minority class\n",
    "minority_class_size = len(class_1)\n",
    "\n",
    "# Sample the majority class to match the size of the minority class\n",
    "undersampled_class_0 = class_0.sample(n=minority_class_size, random_state=42)\n",
    "\n",
    "# Concatenate the undersampled majority class with the minority class\n",
    "undersampled_df = pd.concat([undersampled_class_0, class_1])\n",
    "\n",
    "# Shuffle the rows\n",
    "undersampled_df = undersampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save the undersampled dataframe to a new CSV file\n",
    "undersampled_df.to_csv('my_datasets/dontpatronizeme_v1.4/synthetic/synthetic_pcl_undersampled.csv', index=False)  # Replace 'undersampled_dataset.csv' with your desired file name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1fd0a97-8103-492e-a902-dd1c37e54f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Syrian government has reportedly begun to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sustainable Development Goals: A Decade of Pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The government's decision to deport a group of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The State Department is offering up to $65,000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43753</th>\n",
       "      <td>The European Commission has announced a plan t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43754</th>\n",
       "      <td>The BBC has a long history of promoting mass i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43755</th>\n",
       "      <td>The world is watching as the U.S. government s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43756</th>\n",
       "      <td>The Syrian government has reportedly begun to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43757</th>\n",
       "      <td>The German government has decided to suspend t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43758 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0\n",
       "0                                                      1\n",
       "1      The Syrian government has reportedly begun to ...\n",
       "2      Sustainable Development Goals: A Decade of Pro...\n",
       "3      The government's decision to deport a group of...\n",
       "4      The State Department is offering up to $65,000...\n",
       "...                                                  ...\n",
       "43753  The European Commission has announced a plan t...\n",
       "43754  The BBC has a long history of promoting mass i...\n",
       "43755  The world is watching as the U.S. government s...\n",
       "43756  The Syrian government has reportedly begun to ...\n",
       "43757  The German government has decided to suspend t...\n",
       "\n",
       "[43758 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hn = pd.read_csv(\"my_datasets/HatEval/synthetic/new_HatEval_non_hateful_synthetic_data.csv\", header=None)\n",
    "df_h = pd.read_csv(\"my_datasets/HatEval/synthetic/new_HatEval_hateful_synthetic_data.csv\", header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f664292b-3a96-4ab5-a7e4-072482a7f7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20016/3044752503.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  combined_df = combined_df.append(data, ignore_index=True)\n",
      "/tmp/ipykernel_20016/3044752503.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  combined_df = combined_df.append(data, ignore_index=True)\n",
      "/tmp/ipykernel_20016/3044752503.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  combined_df = combined_df.append(data, ignore_index=True)\n",
      "/tmp/ipykernel_20016/3044752503.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  combined_df = combined_df.append(data, ignore_index=True)\n",
      "/tmp/ipykernel_20016/3044752503.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  combined_df = combined_df.append(data, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "file_paths = ['Logs/Founta/Founta_BERT_composite_metrics_fold_0.pickle', 'Logs/Founta/Founta_BERT_composite_metrics_fold_1.pickle', \n",
    "              'Logs/Founta/Founta_BERT_composite_metrics_fold_2.pickle', 'Logs/Founta/Founta_BERT_composite_metrics_fold_3.pickle', \n",
    "              'Logs/Founta/Founta_BERT_composite_metrics_fold_4.pickle']\n",
    "\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        # Assuming data is a DataFrame, you can append it to the combined DataFrame\n",
    "        combined_df = combined_df.append(data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "174c8f39-8d2f-4655-af5f-08b1f113686c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.477077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.478759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.740554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.770155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.786847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   F1 Score\n",
       "0  0.477077\n",
       "1  0.478759\n",
       "2  0.740554\n",
       "3  0.770155\n",
       "4  0.786847"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45c43636-b727-4d85-8269-53053f19d992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2339/547916652.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfx = df1.append(df2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>\"Media Blackout: DACA Recipient Threatens To C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>ModriÄ_x008d_’s grandfather and relatives were...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>#BuildThatWall#StandWithICE#AmericaFirstPolice...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>Pocket Panini Stovetop Sandwich Maker #diet #h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989</th>\n",
       "      <td>@MakeupWhoreder Bitch RT me one more time &amp; no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>@JeaneHasSpoken @TonjaWalker @livinbythelake @...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>GET THIS STRAIGHT.It's our only hope to save o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>@BusterUSMC @EricJoh37288055 It was like BHO r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>@greggutfeld Shut the fuck up you pussy boy!!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Antonio Sabato JR: I'm An Immigrant &amp;amp; I Su...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  labels\n",
       "555   \"Media Blackout: DACA Recipient Threatens To C...       1\n",
       "491   ModriÄ_x008d_’s grandfather and relatives were...       0\n",
       "527   #BuildThatWall#StandWithICE#AmericaFirstPolice...       0\n",
       "925   Pocket Panini Stovetop Sandwich Maker #diet #h...       0\n",
       "2989  @MakeupWhoreder Bitch RT me one more time & no...       1\n",
       "...                                                 ...     ...\n",
       "1130  @JeaneHasSpoken @TonjaWalker @livinbythelake @...       0\n",
       "1294  GET THIS STRAIGHT.It's our only hope to save o...       0\n",
       "860   @BusterUSMC @EricJoh37288055 It was like BHO r...       0\n",
       "507     @greggutfeld Shut the fuck up you pussy boy!!!!       0\n",
       "174   Antonio Sabato JR: I'm An Immigrant &amp; I Su...       0\n",
       "\n",
       "[4000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df1 = pd.read_csv(\"my_datasets/HatEval/classified_trunc_new_HatEval_non_hateful_synthetic_data.csv\")\n",
    "df1 = pd.read_csv(\"my_datasets/HatEval/original/hateval2019_en_test.csv\")\n",
    "df2 = pd.read_csv(\"my_datasets/HatEval/original/hateval2019_en_dev.csv\")\n",
    "dfx = df1.append(df2)\n",
    "df = dfx.sample(frac=1)\n",
    "df.rename(columns={\"HS\": \"labels\"}, inplace=True)\n",
    "df = df.drop(columns=[\"id\", \"TR\", \"AG\"])\n",
    "df = df[df.columns[~df.columns.str.contains('Unnamed')]]\n",
    "\n",
    "df.to_csv(\"my_datasets/HatEval/processed_HE_train_en.csv\", index=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18fbd162-5310-40d7-a7c4-364295e59f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the scores based on the threshold\n",
    "filtered_hateval_hate = df2[(df2['Label'] == 'LABEL_1') & (df2['Score'] >= 0.7)]\n",
    "# Separate the scores based on the threshold\n",
    "filtered_hateval_non_hate = df1[(df1['Label'] == 'LABEL_0') & (df1['Score'] >= 0.7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28dee4e0-af5b-4cfa-b0a5-25e9ba654b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47/2688153103.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  composite_synthetic_hateval_filtered = filtered_hateval_hate.append(filtered_hateval_non_hate)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22512</th>\n",
       "      <td>The # State of # Immigration in # Europe : A R...</td>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>0.999811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7698</th>\n",
       "      <td>The # WalkAway movement is growing. The latest...</td>\n",
       "      <td>LABEL_1</td>\n",
       "      <td>0.993466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17534</th>\n",
       "      <td>The European Union has a migrant crisis, and i...</td>\n",
       "      <td>LABEL_1</td>\n",
       "      <td>0.982292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34395</th>\n",
       "      <td>The first time I saw a woman with a beard I wa...</td>\n",
       "      <td>LABEL_1</td>\n",
       "      <td>0.998053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28111</th>\n",
       "      <td>Este programa es para los que no tienen ni put...</td>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>0.999631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34097</th>\n",
       "      <td>Theresa May is so desperate to stop the Windru...</td>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>0.999682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33499</th>\n",
       "      <td>The German government is trying to stop the mi...</td>\n",
       "      <td>LABEL_1</td>\n",
       "      <td>0.997687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38885</th>\n",
       "      <td>The U. S. government has approved a plan to re...</td>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>0.999745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38933</th>\n",
       "      <td>The Spanish government has agreed to take in m...</td>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>0.999795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27823</th>\n",
       "      <td>The New York Times reports : https : / / t. co...</td>\n",
       "      <td>LABEL_0</td>\n",
       "      <td>0.999800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62294 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text    Label     Score\n",
       "22512  The # State of # Immigration in # Europe : A R...  LABEL_0  0.999811\n",
       "7698   The # WalkAway movement is growing. The latest...  LABEL_1  0.993466\n",
       "17534  The European Union has a migrant crisis, and i...  LABEL_1  0.982292\n",
       "34395  The first time I saw a woman with a beard I wa...  LABEL_1  0.998053\n",
       "28111  Este programa es para los que no tienen ni put...  LABEL_0  0.999631\n",
       "...                                                  ...      ...       ...\n",
       "34097  Theresa May is so desperate to stop the Windru...  LABEL_0  0.999682\n",
       "33499  The German government is trying to stop the mi...  LABEL_1  0.997687\n",
       "38885  The U. S. government has approved a plan to re...  LABEL_0  0.999745\n",
       "38933  The Spanish government has agreed to take in m...  LABEL_0  0.999795\n",
       "27823  The New York Times reports : https : / / t. co...  LABEL_0  0.999800\n",
       "\n",
       "[62294 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composite_synthetic_hateval_filtered = filtered_hateval_hate.append(filtered_hateval_non_hate)\n",
    "df = composite_synthetic_hateval_filtered.sample(frac=1)\n",
    "df#.to_csv(\"my_datasets/Davidson/Davidson_synthetic_hateful_filtered.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39bf4921-5602-4ffb-abe2-794dd28fe1cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_dataset' from 'transformers' (/opt/conda/lib/python3.9/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mroberta_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m test_filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_datasets/HatEval/processed_HE_test_en.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment/hateval_roberta\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# \"models/HatEval_bert\"\u001b[39;00m\n",
      "File \u001b[0;32m~/master/roberta_functions.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m precision_recall_fscore_support, accuracy_score\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     RobertaTokenizer,\n\u001b[1;32m     10\u001b[0m     RobertaForSequenceClassification,\n\u001b[1;32m     11\u001b[0m     Trainer,\n\u001b[1;32m     12\u001b[0m     TrainingArguments,\n\u001b[1;32m     13\u001b[0m     load_dataset,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m RobertaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroberta-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow_confusion_matrix\u001b[39m(conf_matrix, normalized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, class_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_dataset' from 'transformers' (/opt/conda/lib/python3.9/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from roberta_functions import *\n",
    "\n",
    "test_filepath = \"my_datasets/HatEval/processed_HE_test_en.csv\"\n",
    "model_path = \"experiment/hateval_roberta\"  # \"models/HatEval_bert\"\n",
    "\n",
    "y_pred = load_predict_testset(test_filepath, model_path=model_path, args=TRAINING_ARGS)\n",
    "# We want the macro f1 here\n",
    "print(compute_test_metrics(y_pred, 'macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6692034-8f93-413e-bf9a-5b956f50ea40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Signifikanztests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "72cab340-00e5-4a2a-aa55-fad8079e4bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original mean: epoch                       5.000000\n",
      "eval_accuracy               0.815515\n",
      "eval_f1                     0.814549\n",
      "eval_loss                   1.014650\n",
      "eval_precision              0.814653\n",
      "eval_recall                 0.814867\n",
      "eval_runtime                4.274340\n",
      "eval_samples_per_second    74.304000\n",
      "eval_steps_per_second       9.358200\n",
      "dtype: float64  Original std: epoch                      0.000000\n",
      "eval_accuracy              0.019207\n",
      "eval_f1                    0.018910\n",
      "eval_loss                  0.161519\n",
      "eval_precision             0.019134\n",
      "eval_recall                0.019055\n",
      "eval_runtime               0.015316\n",
      "eval_samples_per_second    0.195001\n",
      "eval_steps_per_second      0.033297\n",
      "dtype: float64 Synthetic mean: Unnamed: 0                  2.000000\n",
      "eval_loss                   2.946007\n",
      "eval_accuracy               0.744313\n",
      "eval_f1                     0.734406\n",
      "eval_precision              0.782966\n",
      "eval_recall                 0.744509\n",
      "eval_runtime                4.124240\n",
      "eval_samples_per_second    77.008400\n",
      "eval_steps_per_second       9.698600\n",
      "epoch                       5.000000\n",
      "dtype: float64, Synth std: Unnamed: 0                 1.581139\n",
      "eval_loss                  0.371524\n",
      "eval_accuracy              0.019008\n",
      "eval_f1                    0.016343\n",
      "eval_precision             0.012365\n",
      "eval_recall                0.009179\n",
      "eval_runtime               0.011778\n",
      "eval_samples_per_second    0.176354\n",
      "eval_steps_per_second      0.027510\n",
      "epoch                      0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "'''\n",
    "Original\n",
    "'''\n",
    "df_orig = pd.read_csv(\"Logs/PCL/PCL_bert_uncased_us_metrics.csv\")\n",
    "'''\n",
    "Synthetic\n",
    "'''\n",
    "df_synth = pd.read_csv(\"Logs/PCL/Bert-synthetic-PCL-us-metrics.csv\")\n",
    "\n",
    "print(f\"Original mean: {df_orig.mean()}  Original std: {df_orig.std()} Synthetic mean: {df_synth.mean()}, Synth std: {df_synth.std()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fba60852-51ca-42c9-b9b5-539e1a815329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_accuracy:\n",
      "   t-statistic: 7.362689555039922\n",
      "   p-value: 0.0018129990283412226\n",
      "   Significant difference\n",
      "eval_precision:\n",
      "   t-statistic: 3.0571246962822425\n",
      "   p-value: 0.037764373489443254\n",
      "   Significant difference\n",
      "eval_recall:\n",
      "   t-statistic: 8.253528491406941\n",
      "   p-value: 0.001175552543626658\n",
      "   Significant difference\n",
      "eval_f1:\n",
      "   t-statistic: 9.624338281062121\n",
      "   p-value: 0.0006516876950050328\n",
      "   Significant difference\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Assuming the columns in your CSV are 'accuracy', 'precision', 'recall', 'f1'\n",
    "evaluation_metrics = ['eval_accuracy', 'eval_precision', 'eval_recall', 'eval_f1']\n",
    "\n",
    "# Initialize dictionaries to store t-test results\n",
    "t_test_results = {}\n",
    "\n",
    "# Perform a paired t-test for each evaluation metric\n",
    "for metric in evaluation_metrics:\n",
    "    t_statistic, p_value = ttest_rel(df_orig[metric], df_synth[metric])\n",
    "    t_test_results[metric] = {'t_statistic': t_statistic, 'p_value': p_value}\n",
    "\n",
    "# Print t-test results\n",
    "for metric, results in t_test_results.items():\n",
    "    print(f\"{metric}:\")\n",
    "    print(f\"   t-statistic: {results['t_statistic']}\")\n",
    "    print(f\"   p-value: {results['p_value']}\")\n",
    "    if results['p_value'] < 0.05:\n",
    "        print(\"   Significant difference\")\n",
    "    else:\n",
    "        print(\"   No significant difference\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b011a6ee-af0d-4a38-af3f-cf15bc96d8bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Similarity:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eacb649-5d46-4c49-af25-13e3af633d01",
   "metadata": {},
   "source": [
    "## Lemmatized words count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46e46de2-4445-4f7f-80e6-b8080cc1bf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique, lemmatized words: 40738\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK resources if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('my_datasets/GermEval/synthetic/composite_GE_dataset.csv')  # Replace 'your_file.csv' with the path to your file\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize, remove stopwords, and lemmatize the text\n",
    "all_words = []\n",
    "for text in df['text']:\n",
    "    text = str(text)  # Ensure that the value is a string\n",
    "    words = word_tokenize(text.lower())  # Convert text to lowercase and tokenize\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in stop_words]\n",
    "    all_words.extend(words)\n",
    "\n",
    "# Get unique lemmatized words\n",
    "unique_lemmatized_words = set(all_words)\n",
    "\n",
    "# Print the count of unique lemmatized words\n",
    "print(f\"Total number of unique, lemmatized words: {len(unique_lemmatized_words)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe1799d-5ba1-4619-aa81-5bdbd525b6e3",
   "metadata": {},
   "source": [
    "## Cosine similarity within label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9070cd2-a91e-4027-9a3b-a537c53dae24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cosine similarity within label '0': 0.25832354274288866\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('my_datasets/Davidson/davidson_original_train.csv')\n",
    "\n",
    "# Filter the data for label '0'\n",
    "label_data = df[df['labels'] == 1]['text'].tolist()\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Compute sentence embeddings\n",
    "embeddings = model.encode(label_data)\n",
    "\n",
    "# Convert embeddings to numpy arrays (if they aren't already)\n",
    "if isinstance(embeddings[0], torch.Tensor):\n",
    "    embeddings = [embedding.cpu().numpy() for embedding in embeddings]\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarities = []\n",
    "for i in range(len(embeddings) - 1):\n",
    "    for j in range(i + 1, len(embeddings)):\n",
    "        similarity = cosine_similarity(embeddings[i].reshape(1, -1), embeddings[j].reshape(1, -1))\n",
    "        similarities.append(similarity[0][0])\n",
    "\n",
    "average_similarity = sum(similarities) / len(similarities)\n",
    "\n",
    "print(f\"Average cosine similarity within label '0': {average_similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4710940-b6b9-498b-8a33-5a5a5a270234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 13:51:48.333743: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-19 13:51:48.839812: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Processing label 0: 100%|██████████| 28724410/28724410 [1:03:13<00:00, 7572.14it/s]\n",
      "Processing label 1: 100%|██████████| 314821/314821 [00:41<00:00, 7530.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cosine similarity within label '0': 0.11210392360347139\n",
      "Average cosine similarity within label '1': 0.18533642170345807\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_average_similarity(df, label):\n",
    "    # Filter the data for the given label\n",
    "    label_data = df[df['labels'] == label]['text'].tolist()\n",
    "\n",
    "    # Compute sentence embeddings\n",
    "    embeddings = model.encode(label_data)\n",
    "\n",
    "    # Convert embeddings to numpy arrays (if they aren't already)\n",
    "    if isinstance(embeddings[0], torch.Tensor):\n",
    "        embeddings = [embedding.cpu().numpy() for embedding in embeddings]\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    similarities = []\n",
    "    total_comparisons = len(embeddings) * (len(embeddings) - 1) // 2\n",
    "    with tqdm(total=total_comparisons, desc=f\"Processing label {label}\") as pbar:\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            for j in range(i + 1, len(embeddings)):\n",
    "                similarity = cosine_similarity(embeddings[i].reshape(1, -1), embeddings[j].reshape(1, -1))\n",
    "                similarities.append(similarity[0][0])\n",
    "                pbar.update(1)\n",
    "\n",
    "    average_similarity = sum(similarities) / len(similarities)\n",
    "    return average_similarity\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('my_datasets/dontpatronizeme_v1.4/train_data.csv')\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Compute average cosine similarity for each label\n",
    "avg_sim_label_0 = compute_average_similarity(df, 0)\n",
    "avg_sim_label_1 = compute_average_similarity(df, 1)\n",
    "\n",
    "print(f\"Average cosine similarity within label '0': {avg_sim_label_0}\")\n",
    "print(f\"Average cosine similarity within label '1': {avg_sim_label_1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884ea996-5360-4527-8257-316420dfb1b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SMOTE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ec74be33-470e-404f-b3c6-3dacbd2bc59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    43080\n",
      "1     3972\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load the dataset\n",
    "original_df = pd.read_csv('my_datasets/Founta/founta_original_train_full.csv')\n",
    "synthetic_df = pd.read_csv('my_datasets/Founta/founta_synth_cleaned.csv')\n",
    "\n",
    "#Step 2: Calculate the class distribution to find the minority class.\n",
    "\n",
    "# calculate the class distribution\n",
    "print(original_df['labels'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "524ebad0-938b-47a3-ba37-d197ec6a714d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    9240\n",
      "0    9240\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter minority class in original and synthetic dataset\n",
    "original_minority_df = original_df[original_df['labels'] == 1]\n",
    "synthetic_minority_df = synthetic_df[synthetic_df['labels'] == 1]\n",
    "\n",
    "# 'count_class_0' corresponds to the number of occurrences of class 0\n",
    "count_class_0 = original_df['labels'].value_counts()[0]\n",
    "\n",
    "# 'count_class_1' corresponds to the number of occurrences of class 1\n",
    "count_class_1 = original_df['labels'].value_counts()[1]\n",
    "\n",
    "# Find out how many more '1' records we need to make the number of '1' class the same as '0' class for balanced classes.\n",
    "additional_samples_required = count_class_0 - count_class_1\n",
    "\n",
    "# If there are not enough synthetic samples, sample all\n",
    "n_samples = min(additional_samples_required, len(synthetic_minority_df))\n",
    "\n",
    "random_samples_df = synthetic_minority_df.sample(n=n_samples, replace=False, random_state=1)\n",
    "\n",
    "# Concatenate original data with sampled synthetic data\n",
    "combined_minority_df = pd.concat([original_minority_df, random_samples_df])\n",
    "\n",
    "# Calculate the new number to under-sample the majority class to\n",
    "undersample_target = len(combined_minority_df)\n",
    "\n",
    "# under-sample majority class\n",
    "original_majority_df = original_df[original_df['labels'] == 0]\n",
    "undersampled_majority_df = original_majority_df.sample(n=undersample_target, random_state=1)\n",
    "\n",
    "# Create a balanced dataframe\n",
    "balanced_df = pd.concat([combined_minority_df, undersampled_majority_df])\n",
    "\n",
    "balanced_df = balanced_df.sample(frac=1)\n",
    "\n",
    "# Verify that the classes are balanced\n",
    "print(balanced_df['labels'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "af446ea1-e6f5-49bd-b427-7c32226360a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df.to_csv(\"my_datasets/SMOTE-Founta.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
